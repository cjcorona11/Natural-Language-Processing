# nlp-classifier-project

In this project, we build a basic NLP classifier to separate "fake" news from "real" news. The motivation for this project comes from George McIntire's article "How to Build a 'Fake News' Classification Model". The data set is a collection of 6,335 articles from various web sources published in the month leading up to the 2016 presidential election (source: https://github.com/GeorgeMcIntire/fake_real_news_dataset). For each article, the data set contains a unique ID, the article's title, the article's text, and the response variable as to whether the article is "real" or "fake" news. We begin by preprocessing the corpus of text. We tokenize individual words, remove punctuation, and convert all words to lowercase. Next we filter out stop words and then perform stemming and lemmatization which transforms all words into their root. The final step in preprocessing is to vectorize the text into a numerical format that our models can use. This means generating a matrix where each row is an article and each column is a possible word from the dictionary. The matrix counts how many of each word are found in the given article article. We also decide to experiment with n-grams, or n-sized word groupings. We evaluate all our models on the predictor set of 1-grams; 1- and 2-grams; and 1-, 2-, and 3-grams. We fit four types of models: logistic regression, naive bayes, random forest, and neural net. We give the results, assessing model performance with F1-scores.
